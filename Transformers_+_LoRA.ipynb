{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOu14JfFRNwxHEcK7t8NzmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwgb93/EdgeRunnerAI-Transformers-LoRA/blob/main/Transformers_%2B_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dr. Dylan's Intro to Transformers\n"
      ],
      "metadata": {
        "id": "ZJkPVqG8onu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizers"
      ],
      "metadata": {
        "id": "ZVtkInAoo1UR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qczw6h3KWDyU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"NousResearch/Meta-Llama-3.1-8B-Instruct\" # So we don't have to deal with gated models\n",
        "#model_name = \"Qwen/Qwen2.5-72B-Instruct\" # Try uncommenting these and comparing the results!\n",
        "#model_name = \"deepseek-ai/DeepSeek-R1\"\n",
        "#model_name = \"openai-community/gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"EdgeRunner AI is a cool place to work!\""
      ],
      "metadata": {
        "id": "44w3BW6bW7fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "print(tokens)\n",
        "print(tokenizer.decode(tokens))"
      ],
      "metadata": {
        "id": "NTjQVJySWGSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello Hello hello\""
      ],
      "metadata": {
        "id": "pMt8t7JOjCe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "print(tokens)\n",
        "print(tokenizer.decode(tokens))"
      ],
      "metadata": {
        "id": "WyoOuaGRjCgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are the tokens different? Try this"
      ],
      "metadata": {
        "id": "iDw2VLPQs7uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(tokens))"
      ],
      "metadata": {
        "id": "du-hedjLybYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we mix and match tokenizers?"
      ],
      "metadata": {
        "id": "5LeF2qe16jDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3.1-8B-Instruct\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-72B-Instruct\") # Replace these if you want"
      ],
      "metadata": {
        "id": "92L-q37a6re0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer1(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "print(tokens)\n",
        "print(tokenizer2.decode(tokens))"
      ],
      "metadata": {
        "id": "mysjvD1Q6h_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "JZy0nWsOs0Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WLuUHYEIuN_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you restarted the notebook, you'll have to run this again"
      ],
      "metadata": {
        "id": "E-DajtO3G9PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "f2YasMbRG5fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/Llama-3.2-1B-Instruct\" # Try a different model like 'gpt2'\n",
        "\n",
        "pipe = pipeline('feature-extraction', model=model_name)\n",
        "data = pipe(\"this is a test\")\n",
        "print(data)\n",
        "print(f\"This text is {len(data[0])} tokens long\")\n",
        "print(f\"Each token is {len(data[0][0])} dimensions long\")"
      ],
      "metadata": {
        "id": "HnSonijGs-HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = np.array(pipe(\"man\"))[0][-1].reshape(1, -1) # Ugh, it returns a list, then the array is the wrong size. There's definitely a better way to do this, lol\n",
        "data2 = np.array(pipe(\"king\"))[0][-1].reshape(1, -1)\n",
        "difference_man = data1 - data2\n",
        "print(f\"Cosine similarity: {cosine_similarity(data1, data2)}\")"
      ],
      "metadata": {
        "id": "xvmzglmQuWmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = np.array(pipe(\"woman\"))[0][-1].reshape(1, -1)\n",
        "data4 = np.array(pipe(\"queen\"))[0][-1].reshape(1, -1)\n",
        "difference_woman = data3 - data4\n",
        "print(f\"Cosine similarity: {cosine_similarity(data3, data4)}\")"
      ],
      "metadata": {
        "id": "8IQ9JTQEuCye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the similarity of `difference_man` and `difference_woman`"
      ],
      "metadata": {
        "id": "5INWD7yRu22c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine similarity: {cosine_similarity(difference_man, difference_woman)}\")"
      ],
      "metadata": {
        "id": "z2eEC6w1u2Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is the result what you expected?\n",
        "\n",
        "Try comparing man to woman and king to queen.\n",
        "\n",
        "Is there a stronger direction for \"gender\" or \"royalty\"?"
      ],
      "metadata": {
        "id": "Pcyw5vCQ2g8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "Adapted from [Hands On Large Language Models - Chapter 3](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/README.md)"
      ],
      "metadata": {
        "id": "7pyE4Xx3WG5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "#model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "#model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "#model_name = \"Qwen/Qwen3-1.7B\"\n",
        "#model_name = \"microsoft/Phi-4-mini-instruct\""
      ],
      "metadata": {
        "id": "Pafhds8pnado"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")"
      ],
      "metadata": {
        "id": "_0EPwuAZkNYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write an email apologizing to Evelyn for the tragic gardening mishap. Explain how it happened.\"}\n",
        "]\n",
        "\n",
        "# Generate the output\n",
        "output = generator(messages)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "NCstBWbbkNad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "0lPuRTtvkNcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and print a few more models. What similarities do you notice? What differences?"
      ],
      "metadata": {
        "id": "neYYS4slofF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "vARrNI1BkNe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "-ruf6viRkNhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling tokens"
      ],
      "metadata": {
        "id": "MXtxfVolOo9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Send them to teh GPU\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# Get the output of the model before the lm_head\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "# Get the output of the lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ],
      "metadata": {
        "id": "DDeCJFqq00rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "print(token_id)\n",
        "print(tokenizer.decode(token_id))"
      ],
      "metadata": {
        "id": "YBP51XgHPtKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output[0].shape"
      ],
      "metadata": {
        "id": "wGmp9VMF1Fxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "id": "iY03lO5u1HqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Playing with temperature"
      ],
      "metadata": {
        "id": "Jx63bgJK2L4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# Generate the output\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "cLoZk28h2jGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply prompt template\n",
        "prompt = generator.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "Bgwj3Fgl2WJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature - run this a couple of times. Are there any differences?\n",
        "output = generator(messages, do_sample=True, temperature=1)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "15IvHeS12nGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the temperature and try again\n",
        "output = generator(messages, do_sample=True, temperature=)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "HWuC2ImP2uVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice?"
      ],
      "metadata": {
        "id": "VaWx1Y5Q21s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a Transformer from Scratch\n",
        "\n",
        "From [nanoGPT](https://github.com/karpathy/nanoGPT) and [Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
      ],
      "metadata": {
        "id": "GjtMjk070Md6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "sKy47Oyd3YsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"nanoGPT\")"
      ],
      "metadata": {
        "id": "u31G2FzS0PG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Shakespeare"
      ],
      "metadata": {
        "id": "VHXUb5Zs3tnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "id": "nsAKNCCC3vk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/nanoGPT/data/shakespeare_char/input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "l-qspNwg4Oax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "392nFPBXCGR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "id": "H6TYKhfsCGUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "id": "7191ltRzCGXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "ER6wJANVCSoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "id": "ihAmnNSXCSqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "id": "WFUrB3QDCfeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train a (smol) GPT to make text like this.\n",
        "\n",
        "Note: This will take several minutes to run."
      ],
      "metadata": {
        "id": "cNGciCcb6YcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --compile=False # needed for colab T4\n"
      ],
      "metadata": {
        "id": "sVDPM7tI4hSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't have a GPU in Colab for whatever reason, try this:"
      ],
      "metadata": {
        "id": "0XmELRWJ6KKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
      ],
      "metadata": {
        "id": "GR5UWcoY6Ebv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did it do?"
      ],
      "metadata": {
        "id": "an51Rn-i6tLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char --num_samples=3"
      ],
      "metadata": {
        "id": "xXsncP4t5Cxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we wanted to train a real model with real data?"
      ],
      "metadata": {
        "id": "ledmY_3R7XJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"data/HarryPotter\")\n",
        "os.chdir(\"data/HarryPotter\")"
      ],
      "metadata": {
        "id": "8ppjk3qw9H8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data"
      ],
      "metadata": {
        "id": "Ait2CmFG9zyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.getcwd(), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://gist.githubusercontent.com/cmaspi/41e1d8e552a30a6d5ef0be7e574da513/raw/0a9a8247da3468a7a40edc2c62479df208c421d9/Harry_Potter_all_books_preprocessed.txt'\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.getcwd(), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.getcwd(), 'val.bin'))\n",
        "\n",
        "# train.bin has 301,966 tokens\n",
        "# val.bin has 36,059 tokens\n"
      ],
      "metadata": {
        "id": "ALITalNT8Uxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, our dataset is about the same size, but the gpt2-xl is ~150x bigger. gpt-medium is still 35x bigger.\n",
        "\n",
        "Maybe we should try `--init_from='gpt2'` instead"
      ],
      "metadata": {
        "id": "z-vEUlVtAEf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"../..\")\n",
        "!pwd\n",
        "\n",
        "# Make sure this says /content/nanoGPT"
      ],
      "metadata": {
        "id": "XnPGwKygAxJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you run out of VRAM during evaluation, reduce the batch_size or --init_from='gpt2'\n",
        "!python train.py --out_dir='out-HP' --eval_interval=5 --eval_iters=40 \\\n",
        "--dataset=\"HarryPotter\" --init_from='gpt2-medium' --always_save_checkpoint=False \\\n",
        "--batch_size=2 --gradient_accumulation_steps=16 --max_iters=20 \\\n",
        "--learning_rate=3e-5 --decay_lr=False --compile=False\n"
      ],
      "metadata": {
        "id": "MKTFCjgD9LhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on what hyperparameters you chose and how long you let this cook, you should be able to recreate Harry Potter books from scratch.\n",
        "\n",
        "That would be an example of overfitting."
      ],
      "metadata": {
        "id": "2i00B3Q5Dj-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-HP --num_samples=2 --start=\"\""
      ],
      "metadata": {
        "id": "rYmPmrXP-nBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with Axolotl"
      ],
      "metadata": {
        "id": "XVKDQzH8A-Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider restarting your environment to free up RAM and such."
      ],
      "metadata": {
        "id": "VN3EJAQdD7Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
        "assert (torch.cuda.is_available()==True)"
      ],
      "metadata": {
        "id": "DBauKOuy99eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__ # Should be 2.6.0+cu124 or similar"
      ],
      "metadata": {
        "id": "c01O7zRvEJh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes a while to install. It may ask to restart your session. That's fine. You don't have to run it again."
      ],
      "metadata": {
        "id": "aDkiGGOnH7A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U packaging==23.2 setuptools==75.8.0 wheel ninja"
      ],
      "metadata": {
        "id": "TV-mOeojNH7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-build-isolation axolotl[deepspeed]\n"
      ],
      "metadata": {
        "id": "U2k4MIAyEEhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!axolotl --version # Should be 0.9.1 or similar"
      ],
      "metadata": {
        "id": "_ME-KeudIDa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a config.yaml file\n",
        "\n",
        "If this takes an insanely long time to download (it took me 5 mins), try\n",
        "\n",
        "`base_model: unsloth/Llama-3.2-1B-Instruct`\n",
        "\n",
        "You can probably turn off `load_in_4bit: true` and change `adapter: lora`\n",
        "\n"
      ],
      "metadata": {
        "id": "VyeVTa5ZETW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "base_model: NousResearch/Meta-Llama-3.1-8B-Instruct\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true # This means using QLoRA\n",
        "strict: false\n",
        "\n",
        "chat_template: llama3\n",
        "\n",
        "datasets:\n",
        "  - path: tatsu-lab/alpaca # generic dataset. Feel free to choose your own, if you have one.\n",
        "    type: alpaca\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.05\n",
        "output_dir: ./outputs/lora-out\n",
        "\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "eval_sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_modules_to_save:\n",
        "\n",
        "wandb_project:\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 8\n",
        "micro_batch_size: 1\n",
        "num_epochs: 1\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 5e-5\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: false # Doesn't work in colab\n",
        "sdp_attention: true\n",
        "\n",
        "warmup_steps: 1\n",
        "max_steps: 10\n",
        "evals_per_epoch: 1\n",
        "eval_table_size:\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: <|end_of_text|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "file_path = 'config.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "FKNh8udkEEj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically optional, but it downloads the model and dataset and lets me know if my dataset is borked before beginning training."
      ],
      "metadata": {
        "id": "p368D8w8IPbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!axolotl preprocess config.yaml"
      ],
      "metadata": {
        "id": "QsoVMR53EEm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This could take literal hours."
      ],
      "metadata": {
        "id": "qbzofVNXMl3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!axolotl train config.yaml"
      ],
      "metadata": {
        "id": "Is6c5c7_IOyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!axolotl inference config.yml --lora-model-dir=\"./outputs/lora-out\" --gradio # Technically not necessary, but it creates a nice little website you can share with others.\n"
      ],
      "metadata": {
        "id": "icMBDnESND3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "nfmBaLS0Je3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}